{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724543b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZATION\n",
    "s1 = \"On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fa18445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'a',\n",
       " '$',\n",
       " '50,000',\n",
       " 'mortgage',\n",
       " 'of',\n",
       " '30',\n",
       " 'years',\n",
       " 'at',\n",
       " '8',\n",
       " 'percent',\n",
       " ',',\n",
       " 'the',\n",
       " 'monthly',\n",
       " 'payment',\n",
       " 'would',\n",
       " 'be',\n",
       " '$',\n",
       " '366.88',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e1cde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 =  \"\\\"We beat some pretty good teams to get here,\\\" Slocum said.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4745be93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " 'We',\n",
       " 'beat',\n",
       " 'some',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'teams',\n",
       " 'to',\n",
       " 'get',\n",
       " 'here',\n",
       " ',',\n",
       " \"''\",\n",
       " 'Slocum',\n",
       " 'said',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2667f965",
   "metadata": {},
   "outputs": [],
   "source": [
    " s3 = \"Well, we couldn't have this predictable, cliche-ridden, \\\"Touched by an Angel\\\" (a show creator John Masius worked on) wanna-be if she didn't.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a25cb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Well',\n",
       " ',',\n",
       " 'we',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'this',\n",
       " 'predictable',\n",
       " ',',\n",
       " 'cliche-ridden',\n",
       " ',',\n",
       " '``',\n",
       " 'Touched',\n",
       " 'by',\n",
       " 'an',\n",
       " 'Angel',\n",
       " \"''\",\n",
       " '(',\n",
       " 'a',\n",
       " 'show',\n",
       " 'creator',\n",
       " 'John',\n",
       " 'Masius',\n",
       " 'worked',\n",
       " 'on',\n",
       " ')',\n",
       " 'wanna-be',\n",
       " 'if',\n",
       " 'she',\n",
       " 'did',\n",
       " \"n't\",\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3 = word_tokenize(s3)\n",
    "S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71dd0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Sentence tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0197bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = \"I called Dr. Jones. I called Dr. Jones. I called Dr. Jones. I called Dr. Jones. I called Dr. Jones. I called Dr. Jones.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d518274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'called',\n",
       " 'Dr.',\n",
       " 'Jones',\n",
       " '.',\n",
       " 'I',\n",
       " 'called',\n",
       " 'Dr.',\n",
       " 'Jones',\n",
       " '.',\n",
       " 'I',\n",
       " 'called',\n",
       " 'Dr.',\n",
       " 'Jones',\n",
       " '.',\n",
       " 'I',\n",
       " 'called',\n",
       " 'Dr.',\n",
       " 'Jones',\n",
       " '.',\n",
       " 'I',\n",
       " 'called',\n",
       " 'Dr.',\n",
       " 'Jones',\n",
       " '.',\n",
       " 'I',\n",
       " 'called',\n",
       " 'Dr.',\n",
       " 'Jones',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edbd1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "s12 = (\"Ich muss unbedingt daran denken, Mehl, usw. fur einen \"\n",
    "...        \"Kuchen einzukaufen. Ich muss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88d110b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich',\n",
       " 'muss',\n",
       " 'unbedingt',\n",
       " 'daran',\n",
       " 'denken',\n",
       " ',',\n",
       " 'Mehl',\n",
       " ',',\n",
       " 'usw.',\n",
       " 'fur',\n",
       " 'einen',\n",
       " 'Kuchen',\n",
       " 'einzukaufen',\n",
       " '.',\n",
       " 'Ich',\n",
       " 'muss',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s12, 'german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f729ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Regexp Tokenizer!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d895da51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alas, it has not rained today. When, do you think, will it rain again?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"Alas, it has not rained today. When, do you think, \"\n",
    "...       \"will it rain again?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67c84f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0cb12e37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regexp_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-6d8b3ba5ff2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mregexp_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr'[,\\.\\?!\"]\\s*'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgaps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'regexp_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "regexp_tokenize(Reg1, r'[,\\.\\?!\"]\\s*', gaps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba5c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f92ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
